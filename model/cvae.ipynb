{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfNT-mlFwxVM"
   },
   "source": [
    "# Convolutional Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITZuApL56Mny"
   },
   "source": [
    "The goal of this notebook is to show how to train a Variational Autoencoder (VAE) ([1](https://arxiv.org/abs/1312.6114), [2](https://arxiv.org/abs/1401.4082)) model on the Indonesia SFINCS runs dataset (generated in by the `prepare/prepare.ipynb` notebook). It is based on the [CVAE notebook](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb) from [tensorflow](https://www.tensorflow.org/tutorials/generative/cvae). A VAE is a probabilistic take on the autoencoder. An autoencoder a model that uses dimension reduction to represent higher dimensional and spaced data into a latent vector space, similar to other dimensionality reduction techniques like PCA. Unlike a traditional autoencoder, which maps the input onto a latent vector, a VAE maps the input data into the parameters of a probability distribution, such as the mean and a standard deviation. This approach can also generate data that is variable.\n",
    "In our example rain will not always lead to a flooding event, but sometimes. This variation is also taken into account in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import backend as K\n",
    "import xarray as xr\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the Indonesia dataset\n",
    "Each example is composed of a a number of input variables and an expected flood map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a4fYMGxGhrna"
   },
   "outputs": [],
   "source": [
    "# open the training examples\n",
    "batch_size = 16\n",
    "train_size = 12 * batch_size\n",
    "shuffle_size = 32\n",
    "\n",
    "\n",
    "ds_full = xr.open_zarr(\"../prepare/test-training.zarr\")\n",
    "# ds = ds_full.sel(example=np.arange(0, train_size))\n",
    "ds = ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baart_f/.virtualenvs/py311/lib/python3.11/site-packages/dask/array/reductions.py:616: RuntimeWarning: All-NaN slice encountered\n",
      "  return np.nanmin(x_chunk, axis=axis, keepdims=keepdims)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'M_slices': [0, 1919],\n",
       " 'M_start_stop': [0, 1919],\n",
       " 'N_slices': [0, 1919],\n",
       " 'N_start_stop': [0, 1919],\n",
       " 'distance_to_river': [0.0, 10263.1015625],\n",
       " 'elevation': [-3.0179998874664307, 2030.858642578125],\n",
       " 'example': [0, 5999],\n",
       " 'flood_depth': [0, 6.927290439605713],\n",
       " 'flood_hand': [0, 0.7437315583229065],\n",
       " 'flowdir_cosine_y': [-1.0, 1.0],\n",
       " 'flowdir_sine_y': [-1.0, 1.0],\n",
       " 'is_flooded': [False, True],\n",
       " 'is_permanent_water': [0.0, 1.0],\n",
       " 'manning_values': [0.0, 0.11999999731779099],\n",
       " 'mask': [0.0, 1.0],\n",
       " 'minmax': [0, 1],\n",
       " 'slope': [0.0, 1.1036791801452637],\n",
       " 'soil_moisture': [0.0, 255.0],\n",
       " 'uparea_idw': [0.0, 218.2404022216797],\n",
       " 'x': [0, 127],\n",
       " 'y': [0, 127]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges = {}\n",
    "for var in ds.variables:\n",
    "    ranges[var] = [\n",
    "        ds.variables[var].min().values.item(),\n",
    "        ds.variables[var].max().values.item(),\n",
    "    ]\n",
    "    if var in ('flood_depth', 'flood_hand'):\n",
    "        ranges[var][0] = 0\n",
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soil_moisture',\n",
       " 'flood_hand',\n",
       " 'elevation',\n",
       " 'uparea_idw',\n",
       " 'slope',\n",
       " 'manning_values',\n",
       " 'distance_to_river',\n",
       " 'flowdir_sine_y',\n",
       " 'flowdir_cosine_y',\n",
       " 'is_permanent_water',\n",
       " 'mask']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = pd.read_json(\"../prepare/tensors.json\")\n",
    "inputs = list(tensors.query(\"role == 'input'\")[\"name\"].values)\n",
    "outputs = list(tensors.query(\"role == 'output'\")[\"name\"].values)\n",
    "num_channels = len(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>var</th>\n",
       "      <th>shape</th>\n",
       "      <th>dtype</th>\n",
       "      <th>dims</th>\n",
       "      <th>role</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soil_moisture</td>\n",
       "      <td>cn_avg</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flood_hand</td>\n",
       "      <td>floodmaps_gis</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elevation</td>\n",
       "      <td>elevation_subsidence</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uparea_idw</td>\n",
       "      <td>uparea_idw</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slope</td>\n",
       "      <td>slope</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>manning_values</td>\n",
       "      <td>manning_values</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>distance_to_river</td>\n",
       "      <td>distance_to_river</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flowdir_sine_y</td>\n",
       "      <td>flowdir_sine_y</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flowdir_cosine_y</td>\n",
       "      <td>flowdir_cosine_y</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>is_permanent_water</td>\n",
       "      <td>permanent_water_p75</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>bool</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mask</td>\n",
       "      <td>mask</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>bool</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>input</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>is_flooded</td>\n",
       "      <td>is_flooded</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>output</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>flood_depth</td>\n",
       "      <td>floodmaps</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>float32</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>output</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M_slices</td>\n",
       "      <td>M_slices</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>int64</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>context</td>\n",
       "      <td>0</td>\n",
       "      <td>1073741824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>N_slices</td>\n",
       "      <td>N_slices</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>int64</td>\n",
       "      <td>[x, y]</td>\n",
       "      <td>context</td>\n",
       "      <td>0</td>\n",
       "      <td>1073741824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M_start_stop</td>\n",
       "      <td>M_start_stop</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>int64</td>\n",
       "      <td>[minmax]</td>\n",
       "      <td>context</td>\n",
       "      <td>0</td>\n",
       "      <td>1073741824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>N_start_stop</td>\n",
       "      <td>N_start_stop</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>int64</td>\n",
       "      <td>[minmax]</td>\n",
       "      <td>context</td>\n",
       "      <td>0</td>\n",
       "      <td>1073741824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                   var       shape    dtype      dims  \\\n",
       "0        soil_moisture                cn_avg  [128, 128]  float32    [x, y]   \n",
       "1           flood_hand         floodmaps_gis  [128, 128]  float32    [x, y]   \n",
       "2            elevation  elevation_subsidence  [128, 128]  float32    [x, y]   \n",
       "3           uparea_idw            uparea_idw  [128, 128]  float32    [x, y]   \n",
       "4                slope                 slope  [128, 128]  float32    [x, y]   \n",
       "5       manning_values        manning_values  [128, 128]  float32    [x, y]   \n",
       "6    distance_to_river     distance_to_river  [128, 128]  float32    [x, y]   \n",
       "7       flowdir_sine_y        flowdir_sine_y  [128, 128]  float32    [x, y]   \n",
       "8     flowdir_cosine_y      flowdir_cosine_y  [128, 128]  float32    [x, y]   \n",
       "9   is_permanent_water   permanent_water_p75  [128, 128]     bool    [x, y]   \n",
       "10                mask                  mask  [128, 128]     bool    [x, y]   \n",
       "11          is_flooded            is_flooded  [128, 128]  float32    [x, y]   \n",
       "12         flood_depth             floodmaps  [128, 128]  float32    [x, y]   \n",
       "13            M_slices              M_slices  [128, 128]    int64    [x, y]   \n",
       "14            N_slices              N_slices  [128, 128]    int64    [x, y]   \n",
       "15        M_start_stop          M_start_stop  [128, 128]    int64  [minmax]   \n",
       "16        N_start_stop          N_start_stop  [128, 128]    int64  [minmax]   \n",
       "\n",
       "       role  min         max  \n",
       "0     input    0           1  \n",
       "1     input    0           1  \n",
       "2     input    0         100  \n",
       "3     input    0         200  \n",
       "4     input    0           1  \n",
       "5     input    0         200  \n",
       "6     input    0       40000  \n",
       "7     input   -1           1  \n",
       "8     input   -1           1  \n",
       "9     input    0           1  \n",
       "10    input    0           1  \n",
       "11   output    0           1  \n",
       "12   output    0           3  \n",
       "13  context    0  1073741824  \n",
       "14  context    0  1073741824  \n",
       "15  context    0  1073741824  \n",
       "16  context    0  1073741824  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the netcdf's to tensors.\n",
    "Here we convert and scale our data to a data structure that tensorflow understands. See this [tutorial](https://www.noahbrenowitz.com/post/loading_netcdfs/) for a nice introduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:36:43.133997: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-02-22 14:36:43.134021: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-02-22 14:36:43.134027: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-02-22 14:36:43.134058: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-22 14:36:43.134072: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# tf.convert_to_tensor(ds[\"a\"])\n",
    "tensor_data = {}\n",
    "for i, row in tensors.iterrows():\n",
    "    # floats only\n",
    "    arr = ds[row[\"name\"]].values.astype(\"float32\")\n",
    "    # compute to 0-1\n",
    "    min_i, max_i = ranges[row[\"name\"]]\n",
    "    arr = (arr - min_i) / (max_i - min_i)\n",
    "    # store scaled date\n",
    "    tensor_data[row[\"name\"]] = tf.convert_to_tensor(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ds = tf.data.Dataset.from_tensor_slices(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_example(example):\n",
    "    def replace_nan(t):\n",
    "        \"\"\"replace nans with 0\"\"\"\n",
    "        return tf.where(tf.math.is_nan(t), tf.zeros_like(t), t)\n",
    "    nan_treatment = {\n",
    "        \"flood_hand\": replace_nan,\n",
    "        \"slope\": replace_nan,\n",
    "        \"manning_values\": replace_nan\n",
    "    }\n",
    "    for key, example_i in example.items():\n",
    "        example_i = replace_nan(example_i)\n",
    "        min, max = ranges[key]\n",
    "        scaled_example_i = (example_i - min) / (max - min)\n",
    "        example[key] = scaled_example_i\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xy(example):\n",
    "    # example = list(batch_example)[0]\n",
    "    x = tf.concat([example[x][..., np.newaxis] for x in inputs], axis=-1)\n",
    "    y = example[outputs[1]][..., np.newaxis]\n",
    "    mask = example[\"mask\"][..., np.newaxis]\n",
    "    y = tf.concat([y, mask], axis=-1)\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_i = ds_full.sel(example=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples():\n",
    "    for i in ds_full.example:\n",
    "        ds_i = ds_full.sel(example=i)\n",
    "        # nothing flooded, no need to train\n",
    "        \n",
    "        if (~np.isnan(ds_i['flood_depth'].values)).sum() < 100:\n",
    "            continue\n",
    "            \n",
    "        tensor_data = {}\n",
    "        for i, row in tensors.iterrows():\n",
    "            # floats only\n",
    "            arr = ds_i[row[\"name\"]].values.astype(\"float32\")\n",
    "            # store scaled date\n",
    "            tensor_data[row[\"name\"]] = tf.convert_to_tensor(arr)    \n",
    "\n",
    "        \n",
    "        \n",
    "        example_i = postprocess_example(tensor_data)\n",
    "\n",
    "        n_flooded = (example_i['flood_depth'] > 0).numpy().sum()\n",
    "        if n_flooded < 100:\n",
    "            continue\n",
    "    \n",
    "        x_i, y_i = generate_xy(example_i)\n",
    "        yield x_i, y_i\n",
    "x_spec = tf.TensorSpec(shape=(img_shape[0], img_shape[1], num_channels), dtype=tf.float32, name='x')\n",
    "y_spec = tf.TensorSpec(shape=(img_shape[0], img_shape[1], 2), dtype=tf.float32, name='y')\n",
    "train_dataset = tf.data.Dataset.from_generator(generate_examples, output_signature=(x_spec, y_spec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Use *tf.data* to batch and shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = tf_ds.shuffle(train_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x51d6d2b50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_example = train_dataset.take(1)\n",
    "example_x, example_y = list(batch_example)[0]\n",
    "plt.imshow(example_y[0, ..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Define the encoder and decoder networks with *tf.keras.Sequential*\n",
    "\n",
    "In this VAE example, use two small ConvNets for the encoder and decoder networks. In the literature, these networks are also referred to as inference/recognition and generative models respectively. Use `tf.keras.Sequential` to simplify implementation. Let $x$ and $z$ denote the observation and latent variable respectively in the following descriptions.\n",
    "\n",
    "### Encoder network\n",
    "This defines the approximate posterior distribution $q(z|x)$, which takes as input an observation and outputs a set of parameters for specifying the conditional distribution of the latent representation $z$. \n",
    "In this example, simply model the distribution as a diagonal Gaussian, and the network outputs the mean and log-variance parameters of a factorized Gaussian. \n",
    "Output log-variance instead of the variance directly for numerical stability.\n",
    "\n",
    "### Decoder network \n",
    "This defines the conditional distribution of the observation $p(x|z)$, which takes a latent sample $z$ as input and outputs the parameters for a conditional distribution of the observation.\n",
    "Model the latent distribution prior $p(z)$ as a unit Gaussian.\n",
    "\n",
    "### Reparameterization trick\n",
    "To generate a sample $z$ for the decoder during training, you can sample from the latent distribution defined by the parameters outputted by the encoder, given an input observation $x$.\n",
    "However, this sampling operation creates a bottleneck because backpropagation cannot flow through a random node.\n",
    "\n",
    "To address this, use a reparameterization trick.\n",
    "In our example, you approximate $z$ using the decoder parameters and another parameter $\\epsilon$ as follows:\n",
    "\n",
    "$$z = \\mu + \\sigma \\odot \\epsilon$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ represent the mean and standard deviation of a Gaussian distribution respectively. They can be derived from the decoder output. The $\\epsilon$ can be thought of as a random noise used to maintain stochasticity of $z$. Generate $\\epsilon$ from a standard normal distribution.\n",
    "\n",
    "The latent variable $z$ is now generated by a function of $\\mu$, $\\sigma$ and $\\epsilon$, which would enable the model to backpropagate gradients in the encoder through $\\mu$ and $\\sigma$ respectively, while maintaining stochasticity through $\\epsilon$.\n",
    "\n",
    "### Network architecture\n",
    "For the encoder network, use two convolutional layers followed by a fully-connected layer. In the decoder network, mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VGLbvBEmjK0a"
   },
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, img_shape, latent_dim, num_channels, batch_size):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(\n",
    "                    input_shape=(img_shape[0], img_shape[1], num_channels),\n",
    "                    batch_size=batch_size,\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    padding=\"same\",\n",
    "                    filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "                ),\n",
    "                # consider max pooling, here?\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    padding=\"same\",\n",
    "                    filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "                ),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                # No activation\n",
    "                # tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "                tf.keras.layers.Dense(latent_dim ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "                # why 32\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=img_shape[0] // 4 * img_shape[1] // 4 * 32,\n",
    "                    activation=tf.nn.relu,\n",
    "                ),\n",
    "                tf.keras.layers.Reshape(\n",
    "                    target_shape=(img_shape[0] // 4, img_shape[1] // 4, 32)\n",
    "                ),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"same\",\n",
    "                    activation=\"relu\",\n",
    "                ),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"same\",\n",
    "                    activation=\"relu\",\n",
    "                ),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=3, strides=1, padding=\"same\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \n",
    "        eps = tf.random.normal(shape=(self.batch_size, self.latent_dim))\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def split(self, encoded):\n",
    "        mean, logvar = tf.split(encoded, num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar  \n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        # reparameterization trick\n",
    "        # mean, logvar = self.split(encoded)\n",
    "        # random normal distribution + whatever....\n",
    "        # parameterized = self.reparameterize(mean, logvar)\n",
    "        \n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss function and the optimizer\n",
    "\n",
    "VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:\n",
    "\n",
    "$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n",
    "\n",
    "In practice, optimize the single sample Monte Carlo estimate of this expectation:\n",
    "\n",
    "$$\\log p(x| z) + \\log p(z) - \\log q(z|x),$$\n",
    "where $z$ is sampled from $q(z|x)$.\n",
    "\n",
    "Note: You could also analytically compute the KL term, but here you incorporate all three terms in the Monte Carlo estimator for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(true, pred):\n",
    "    # get last channel as mask\n",
    "    mask = true[..., -1]\n",
    "    mask = mask[..., tf.newaxis]\n",
    "\n",
    "    # the first channel is the true flood level\n",
    "    true = true[..., 0]\n",
    "    true = true[..., tf.newaxis]\n",
    "\n",
    "    # compute the squared error\n",
    "    error = K.square(true - pred)\n",
    "    # replace non masked values (mask==0) with error, masked areas with 0\n",
    "    error = tf.where(K.less_equal(mask, 0.5), error, tf.zeros_like(error))\n",
    "\n",
    "    error = K.mean(error)\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "model = CVAE(img_shape=img_shape, latent_dim=100, num_channels=num_channels, batch_size=batch_size)\n",
    "model.compile(optimizer=\"adam\", loss=loss)\n",
    "input_shape = tf.TensorShape([None, img_shape[0], img_shape[1], num_channels])\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:45:08.056159: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 619s 16s/step - loss: 1.9847e-06\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 544s 17s/step - loss: 1.5767e-06\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 622s 20s/step - loss: 1.4543e-06\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 691s 21s/step - loss: 2.0852e-06\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 705s 22s/step - loss: 1.5634e-06\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 567s 18s/step - loss: 1.5721e-06\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 596s 19s/step - loss: 2.0007e-06\n",
      "Epoch 8/20\n",
      "12/32 [==========>...................] - ETA: 4:32 - loss: 1.1866e-06"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset.repeat(), steps_per_epoch=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.summary()\n",
    "model.decoder.summary()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_sample):\n",
    "    mean, logvar = model.encode(test_sample)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    predictions = model.sample(z)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    plt.savefig(\"image_at_epoch_{:04d}.png\".format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swCyrbqQQ-Ri"
   },
   "outputs": [],
   "source": [
    "# Pick a sample of the test set for generating output images\n",
    "num_examples_to_generate = 16\n",
    "assert batch_size >= num_examples_to_generate\n",
    "for test_batch in train_dataset.take(1):\n",
    "    x, y = test_batch\n",
    "    test_sample = x[0:num_examples_to_generate, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, nrows=4, sharex=True, sharey=True)\n",
    "for i, (example_i, ax) in enumerate(zip(x, axes.flat)):\n",
    "    im = ax.imshow(example_i[..., 8])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 14\n",
    "\n",
    "import ipywidgets\n",
    "y_pred = model.predict(test_sample)\n",
    "\n",
    "@ipywidgets.interact(i=(0, 15))\n",
    "def plot(i):\n",
    "    fig, axes = plt.subplots(ncols=3)\n",
    "    axes[0].imshow(y_pred[i])\n",
    "    axes[0].set_title(f\"{y_pred[i, ..., 0].min():.2f} - {y_pred[i, ..., 0].max():.2f}\")\n",
    "    \n",
    "    axes[1].imshow(y[i, ..., 0])\n",
    "    axes[1].set_title(f\"{y[i, ..., 0].numpy().min()} - {y[i, ..., 0].numpy().max()}\")\n",
    "    axes[2].imshow(x[i, ..., 0])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "generate_and_save_images(model, 0, test_sample)\n",
    "losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for train_x in train_dataset:\n",
    "        train_step(model, train_x, optimizer)\n",
    "    end_time = time.time()\n",
    "\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    losses.append(elbo)\n",
    "    display.clear_output(wait=False)\n",
    "    print(\n",
    "        \"Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}\".format(\n",
    "            epoch, elbo, end_time - start_time\n",
    "        )\n",
    "    )\n",
    "    generate_and_save_images(model, epoch, test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "### Display a generated image from the last training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfO5wCdclHGL"
   },
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open(\"image_at_epoch_{:04d}.png\".format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x3q9_Oe5q0A"
   },
   "outputs": [],
   "source": [
    "plt.imshow(display_image(epoch))\n",
    "plt.axis(\"off\")  # Display images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NywiH3nL8guF"
   },
   "source": [
    "### Display an animated GIF of all the saved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "anim_file = \"cvae.gif\"\n",
    "\n",
    "with imageio.get_writer(anim_file, mode=\"I\") as writer:\n",
    "    filenames = glob.glob(\"image*.png\")\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZqAEtdqUmJF"
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeunRU6TSumT"
   },
   "source": [
    "### Display a 2D manifold of digits from the latent space\n",
    "\n",
    "Running the code below will show a continuous distribution of the different digit classes, with each digit morphing into another across the 2D latent space. Use [TensorFlow Probability](https://www.tensorflow.org/probability) to generate a standard normal distribution for the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "mNcaaYPBS3mj"
   },
   "outputs": [],
   "source": [
    "def plot_latent_images(model, n, digit_size=28):\n",
    "    \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
    "\n",
    "    norm = tfp.distributions.Normal(0, 1)\n",
    "    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    image_width = digit_size * n\n",
    "    image_height = image_width\n",
    "    image = np.zeros((image_height, image_width))\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z = np.array([[xi, yi]])\n",
    "            x_decoded = model.sample(z)\n",
    "            digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
    "            image[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit.numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap=\"Greys_r\")\n",
    "    plt.axis(\"Off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-ZG69QCZnGY"
   },
   "outputs": [],
   "source": [
    "plot_latent_images(model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrJRef8Ln945"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This tutorial has demonstrated how to implement a convolutional variational autoencoder using TensorFlow. \n",
    "\n",
    "As a next step, you could try to improve the model output by increasing the network size. \n",
    "For instance, you could try setting the `filter` parameters for each of the `Conv2D` and `Conv2DTranspose` layers to 512. \n",
    "Note that in order to generate the final 2D latent image plot, you would need to keep `latent_dim` to 2. Also, the training time would increase as the network size increases.\n",
    "\n",
    "You could also try implementing a VAE using a different dataset, such as CIFAR-10.\n",
    "\n",
    "VAEs can be implemented in several different styles and of varying complexity. You can find additional implementations in the following sources:\n",
    "- [Variational AutoEncoder (keras.io)](https://keras.io/examples/generative/vae/)\n",
    "- [VAE example from \"Writing custom layers and models\" guide (tensorflow.org)](https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)\n",
    "- [TFP Probabilistic Layers: Variational Auto Encoder](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE)\n",
    "\n",
    "If you'd like to learn more about the details of VAEs, please refer to [An Introduction to Variational Autoencoders](https://arxiv.org/abs/1906.02691)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cvae.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
